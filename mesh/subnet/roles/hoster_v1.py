import asyncio
import hashlib
import io
import secrets
from typing import Optional, Tuple

import torch

from mesh import DHT, get_dht_time
from mesh.dht.validation import RecordValidatorBase
from mesh.subnet.protocols.inference_protocol import InferenceProtocol
from mesh.subnet.utils.consensus import MAX_HOSTER_COMMIT_TIME, MAX_HOSTER_REVEAL_TIME, get_consensus_key
from mesh.subnet.utils.hoster import (
    get_hoster_commit_key,
    get_hoster_reveal_key,
    get_hoster_subkey_rsa,
)
from mesh.substrate.chain_functions import get_block_number
from mesh.substrate.config import BLOCK_SECS, SubstrateConfigCustom
from mesh.substrate.utils import get_epoch_progress
from mesh.utils.logging import get_logger

logger = get_logger(__name__)

_max_hoster_commit_time = MAX_HOSTER_COMMIT_TIME - 60
_max_hoster_reveal_time = MAX_HOSTER_REVEAL_TIME - 60

class Hoster:
    """
    The Hoster is responsible for participating in a decentralized inference protocol by:

    1. Listening for a random tensor generated by the validator at the start of the epoch.
    2. Running inference on that tensor using a local model (should be same model for each validator in this specific container use case).
    3. Committing a salted hash of the inference result to the DHT during the commit phase (usually first half) of the epoch.
    4. Revealing the full inference result along with the salt during the reveal phase (usually second half) of the same epoch.
        - The commit-reveal is validated by all nodes (see validator.py - Validator class)

    This commit-reveal scheme ensures that hosters cannot copy or fabricate others' outputs after seeing their commits.

    Args:
        dht (DHT): The DHT node for storing and retrieving data.
        inference_protocol (InferenceProtocol): Interface for running streaming inference on input tensors.
        record_validator (RecordValidatorBase): Keypair used for signing DHT records.
        substrate (SubstrateConfigCustom): Substrate client config for accessing block and epoch info.
    """

    def __init__(
        self,
        dht: DHT,
        inference_protocol: InferenceProtocol,
        record_validator: RecordValidatorBase,
        substrate: SubstrateConfigCustom,
    ):
        self.dht = dht
        self.peer_id = self.dht.peer_id
        self.inference_protocol = inference_protocol
        self.record_validator = record_validator
        self.substrate = substrate
        # Derive epoch length in seconds from substrate config (blocks * block interval)
        self.epoch_length = 0
        self.records_extension = 360
        # self.epoch_length = int(str(get_epoch_length(self.substrate.interface)))

    async def try_load_tensor(self, key: bytes) -> Optional[torch.Tensor]:
        """
        Load the validators random tensor for the epoch

        We expect the data to use the recore validator so we expect it to have a
        subkey using their public key and remove that from the value to extract
        the data
        """
        result = self.dht.get(key, latest=True)

        if result is None:
            return None
        try:
            data = next(iter(result.value.values())).value
            return torch.load(io.BytesIO(data), weights_only=False)
        except Exception as e:
            logger.warning(f"Loading tensor failed with: {e}", exc_info=True)
            return None

    def model_commit_fn(self, tensor: torch.Tensor) -> Tuple[bytes, bytes]:
        """
        Create a salted SHA256 hash of the tensor.
        Returns (salt, hash).
        """
        buffer = io.BytesIO()
        torch.save(tensor, buffer)
        tensor_bytes = buffer.getvalue()

        salt = secrets.token_bytes(16)
        digest = hashlib.sha256(salt + tensor_bytes).digest()
        return salt, digest

    def commit(self, epoch: int, result: torch.Tensor):
        salt, digest = self.model_commit_fn(result)
        self.last_salt = salt
        self.dht.store(
            get_hoster_commit_key(epoch),
            digest,
            get_dht_time() + self.epoch_length + self.records_extension,
            get_hoster_subkey_rsa(self.record_validator),
        )
        logger.info(f"[Hoster] Committed hash for epoch {epoch}")

    def reveal(self, epoch: int, result: torch.Tensor):
        buffer = io.BytesIO()
        torch.save(result, buffer)
        payload = {
            "salt": self.last_salt,
            "tensor": buffer.getvalue(),
        }
        """
        asyncio.create_task(
            asyncio.wait_for(
                self.dht.store(
                    download_key,
                    subkey=self.peer_id.to_bytes(),
                    value=self.state_sharing_priority if self.allow_state_sharing else None,
                    expiration_time=expiration_time,
                    return_future=True,
                ),
                timeout=expiration_time - get_dht_time(),
            )
        )
        """
        self.dht.store(
            get_hoster_reveal_key(epoch),
            payload,
            get_dht_time() + self.epoch_length + self.records_extension,
            get_hoster_subkey_rsa(self.record_validator),
        )

        logger.info(f"[Hoster] Revealed reveal for epoch {epoch}")

    async def step(self):
        """
        Check for the consensus tensor and either commit or reveal based on epoch progress.
        This method will be used internally but does not loopâ€”run_forever() orchestrates timing.
        """
        current_block = get_block_number(self.substrate.interface)
        current_epoch = current_block // self.epoch_length
        epoch_data = get_epoch_progress(current_block, current_epoch)
        epoch = epoch_data.epoch

        consensus_key = get_consensus_key(epoch)
        consensus_tensor = await self.try_load_tensor(consensus_key)
        if consensus_tensor is None:
            return None, None  # No work this step

        # Call inference on self
        inference_output = await self.inference_protocol.call_inference_stream(
            peer=self.peer_id,
            promt="",
            tensor=consensus_tensor
        )

        return epoch, inference_output

    async def run_forever(self):
        """
        Main loop:
         1. In the first half of each epoch, wait for the consensus tensor, run inference, commit.
         2. Wait until the second half of the same epoch, then reveal.
         3. Sleep until the next epoch begins, then repeat.
        """
        while True:
            current_block = get_block_number(self.substrate.interface)
            current_epoch = current_block // self.epoch_length
            epoch_data = get_epoch_progress(current_block, current_epoch)
            percent = epoch_data.percent_complete

            result = None

            """
            Step 1: Wait for random prompt to be stored to DHT and commit in the first half of epoch
            """
            if percent <= 0.5:
                while True:
                    epoch_data = get_epoch_progress(current_block, current_epoch)
                    epoch = epoch_data.epoch
                    percent = epoch_data.percent_complete

                    # If we've moved into the second half without finding a tensor, break
                    if percent > 0.5:
                        break

                    consensus_tensor = await self.try_load_tensor(get_consensus_key(epoch), epoch)
                    if consensus_tensor is not None:
                        result = await self.inference_protocol.run_inference_stream(consensus_tensor)
                        self.commit(epoch, result)
                        break

                    await asyncio.sleep(BLOCK_SECS)

            """
            Step 2: Wait until second half of epoch to reveal commit to DHT
            """
            while True:
                current_block = get_block_number(self.substrate.interface)
                current_epoch = current_block // self.epoch_length
                epoch_data = get_epoch_progress(current_block, current_epoch)
                percent = epoch_data.percent_complete
                epoch = epoch_data.epoch

                if percent > 0.5:
                    break

                await asyncio.sleep(1.0)

            if result is not None:
                print(f"[Hoster] revealing for epoch {epoch}")
                self.reveal(epoch, result)
            else:
                print(f"[Hoster] No inference result to reveal for epoch {epoch}")


            """
            Sleep until next epoch
            """
            current_block = get_block_number(self.substrate.interface)
            current_epoch = current_block // self.epoch_length
            blocks_until_next_epoch = ((current_epoch + 1) * self.epoch_length) - current_block
            seconds_until_next_epoch = blocks_until_next_epoch * BLOCK_SECS
            while True:
                await asyncio.sleep(seconds_until_next_epoch)
                if epoch != current_epoch:
                    break
