import asyncio
import hashlib
import io
import secrets
import threading
from typing import Optional, Tuple

import torch

from mesh import DHT, get_dht_time
from mesh.dht.validation import RecordValidatorBase
from mesh.subnet.protocols.inference_protocol import InferenceProtocol
from mesh.subnet.utils.consensus import (
    CONSENSUS_STORE_DEADLINE,
    HOSTER_COMMIT_DEADLINE,
    HOSTER_REVEAL_DEADLINE,
    MAX_HOSTER_COMMIT_TIME,
    MAX_HOSTER_REVEAL_TIME,
    get_consensus_key,
)
from mesh.subnet.utils.hoster import (
    get_hoster_commit_key,
    get_hoster_reveal_key,
    get_hoster_subkey_rsa,
)
from mesh.subnet.utils.key import extract_rsa_peer_id
from mesh.substrate.chain_functions_v2 import Hypertensor
from mesh.substrate.config import BLOCK_SECS
from mesh.substrate.utils import get_epoch_progress
from mesh.utils.logging import get_logger

logger = get_logger(__name__)

# We remove 60 seconds off of the max expiration in case another node gets it late
# Max discrepency in DHT times is about 4 seconds.
_max_hoster_commit_time = MAX_HOSTER_COMMIT_TIME - 60
_max_hoster_reveal_time = MAX_HOSTER_REVEAL_TIME - 60

class Hoster:
    """
    The Hoster is responsible for participating in a decentralized inference protocol by:

    1. Listening for a random tensor generated by the validator at the start of the epoch.
    2. Running inference on that tensor using a local model (should be same model for each validator in this specific container use case).
    3. Committing a salted hash of the inference result to the DHT during the commit phase (usually first half) of the epoch.
    4. Revealing the full inference result along with the salt during the reveal phase (usually second half) of the same epoch.
        - The commit-reveal is validated by all nodes (see validator.py - Validator class)

    This commit-reveal scheme ensures that hosters cannot copy or fabricate others' outputs after seeing their commits.

    Args:
        dht (DHT): The DHT node for storing and retrieving data.
        inference_protocol (InferenceProtocol): Interface for running streaming inference on input tensors.
        record_validator (RecordValidatorBase): Keypair used for signing DHT records.
        hypertensor (Hypertensor): Hypertensor client config for accessing block and epoch info.
    """

    def __init__(
        self,
        dht: DHT,
        subnet_id: int,
        subnet_node_id: int,
        inference_protocol: InferenceProtocol,
        record_validator: RecordValidatorBase,
        hypertensor: Hypertensor,
    ):
        self.dht = dht
        self.subnet_id = subnet_id
        self.subnet_node_id = subnet_node_id
        self.peer_id = self.dht.peer_id
        self.inference_protocol = inference_protocol
        self.record_validator = record_validator
        self.hypertensor = hypertensor
        self.records_extension = 360
        self.epoch_length = int(str(self.hypertensor.get_epoch_length()))
        self.stop = threading.Event()

    async def run(self):
        """
        Main loop:
         1. In the first half of each epoch, wait for the consensus tensor, run inference, commit.
         2. Wait until the second half of the same epoch, then reveal.
         3. Sleep until the next epoch begins, then repeat.
        """
        while not self.stop.is_set():
            epoch_data = self.hypertensor.get_epoch_progress()
            current_block = epoch_data.block
            current_epoch = epoch_data.epoch
            percent_complete = epoch_data.percent_complete
            seconds_per_epoch = epoch_data.seconds_per_epoch
            seconds_elapsed = epoch_data.seconds_elapsed

            time_consensus_store_deadline_remaining = (seconds_per_epoch - seconds_elapsed) * CONSENSUS_STORE_DEADLINE

            # Get reveal time in `time` to avoid using RPC from blockchains
            # This will allow us to track the time because commit may take some time depending on server
            time_hoster_commit_deadline = get_dht_time() + (seconds_per_epoch - seconds_elapsed) * HOSTER_COMMIT_DEADLINE

            # Wait until the prompt deadline to run inference and commit
            await asyncio.sleep(time_consensus_store_deadline_remaining)

            result = None

            """
            Step 1: Wait for random prompt to be stored to DHT and commit in the first half of epoch
            """
            while not self.stop.is_set():
                # Double check time
                epoch_data = get_epoch_progress(current_block, current_epoch)
                epoch = epoch_data.epoch
                percent_complete = epoch_data.percent_complete

                # If we've moved into the second half without finding a tensor, break
                if percent_complete > HOSTER_COMMIT_DEADLINE:
                    break

                # Run inference on random prompt
                result = self.try_commit_inference(epoch)

                # break if successful
                if result:
                    break

                # If commit unsuccessful the tensor may not be ready yet or storage unsuccessful
                # Wait another block and try again
                await asyncio.sleep(1.0)

            """
            Step 2: Wait until second half of epoch to reveal commit to DHT
            """
            # If commit failed â¸º Redundant
            if self.last_inference is None:
                return

            dht_time = get_dht_time()

            # Sleep until reveal phase
            # Avoid using RPC from blockchains
            await asyncio.sleep(
                max(0.0, time_hoster_commit_deadline - dht_time)
            )

            while not self.stop.is_set():
                # Even though we slept, check we are ready to reveal
                epoch_data = get_epoch_progress(current_block, current_epoch)
                current_block = epoch_data.block
                current_epoch = epoch_data.epoch
                percent_complete = epoch_data.percent_complete
                epoch = epoch_data.epoch

                if percent_complete > HOSTER_COMMIT_DEADLINE and percent_complete <= HOSTER_REVEAL_DEADLINE:
                    break

                if self.last_inference is not None:
                    print(f"[Hoster] revealing for epoch {epoch}")
                    result = self.reveal(epoch, result)
                    if result:
                        break
                    else:
                        print(f"[Hoster] Attempting to reveal again for epoch {epoch}")
                else:
                    print(f"[Hoster] No inference result to reveal for epoch {epoch}")
                    break

                await asyncio.sleep(1.0)



    async def try_load_tensor(self, key: bytes, epoch: int) -> Optional[torch.Tensor]:
        """
        Load the validators random tensor for the epoch

        We expect the data to use the recore validator so we expect it to have a
        subkey using their public key and remove that from the value to extract
        the data
        """
        results = self.dht.get(key, latest=True)

        if results is None:
            return None
        try:
            """
            Get submitted torch.Tensor from the validator if exists, otherwise, the next
            """
            print("try_load_tensor results", results)

            validator = self.hypertensor.get_elected_validator_node(self.subnet_id, epoch)
            validator_peer_id = validator["peer_id"]

            first_prompt = None
            validator_prompt = None
            for public_key, data in results.value.items():
                value = data.value
                if first_prompt is None:
                    first_prompt = value

                peer_id = extract_rsa_peer_id(public_key)
                if peer_id is not None and validator_peer_id == peer_id:
                    validator_prompt = value
                    break

            data = validator_prompt or first_prompt

            return torch.load(io.BytesIO(data), weights_only=False)
        except Exception as e:
            logger.warning(f"Loading tensor failed with: {e}", exc_info=True)
            return None

    def model_commit_fn(self, tensor: torch.Tensor) -> Tuple[bytes, bytes]:
        """
        Create a salted SHA256 hash of the tensor.
        Returns (salt, hash).
        """
        buffer = io.BytesIO()
        torch.save(tensor, buffer)
        tensor_bytes = buffer.getvalue()

        salt = secrets.token_bytes(16)
        digest = hashlib.sha256(salt + tensor_bytes).digest()
        return salt, digest

    def commit(self, epoch: int, result: torch.Tensor):
        salt, digest = self.model_commit_fn(result)
        self.last_salt = salt
        self.last_inference = result
        result = self.dht.store(
            get_hoster_commit_key(epoch),
            digest,
            get_dht_time() + _max_hoster_commit_time,
            get_hoster_subkey_rsa(self.record_validator),
        )
        print("HosterV2 commit", result)
        logger.info(f"[Hoster] Committed hash for epoch {epoch}")
        return result

    def reveal(self, epoch: int, result: torch.Tensor):
        buffer = io.BytesIO()
        torch.save(result, buffer)
        payload = {
            "salt": self.last_salt,
            "tensor": buffer.getvalue(),
        }
        """
        asyncio.create_task(
            asyncio.wait_for(
                self.dht.store(
                    download_key,
                    subkey=self.peer_id.to_bytes(),
                    value=self.state_sharing_priority if self.allow_state_sharing else None,
                    expiration_time=expiration_time,
                    return_future=True,
                ),
                timeout=expiration_time - get_dht_time(),
            )
        )
        """
        result = self.dht.store(
            get_hoster_reveal_key(epoch),
            payload,
            get_dht_time() + _max_hoster_reveal_time,
            get_hoster_subkey_rsa(self.record_validator),
        )
        print("HosterV2 reveal", result)

        logger.info(f"[Hoster] Revealed reveal for epoch {epoch}")

        return result

    async def try_commit_inference(self, epoch: int):
        """
        Check for the consensus tensor and either commit or reveal based on epoch progress.
        """

        consensus_key = get_consensus_key(epoch)
        consensus_tensor = await self.try_load_tensor(consensus_key, epoch)
        if consensus_tensor is None:
            return False

        # Call inference on self
        inference_output = await self.inference_protocol.call_inference_stream(
            peer=self.peer_id,
            promt="",
            tensor=consensus_tensor
        )

        return self.commit(epoch, inference_output)

    def shutdown(self):
        self.stop.set()
